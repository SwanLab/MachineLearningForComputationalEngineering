{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNk9GsLoD-es"
   },
   "source": [
    "#PRACTICAL SESSION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Lom93VRh7PN"
   },
   "source": [
    "**PART I LINEAR REGRESSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDk5RVhfEJue"
   },
   "source": [
    "STEP 1: Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nU6EOoUjJVhg"
   },
   "outputs": [],
   "source": [
    "# Important libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import linprog\n",
    "import data_analysis as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LeT6n9E-SMh"
   },
   "source": [
    "STEP 2: Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "Df-o-Nn_27SS",
    "outputId": "342c441f-7bfb-43c7-89ff-5eebddaf65f8"
   },
   "outputs": [],
   "source": [
    "# Lets create a object data and plot it\n",
    "data1 = da.Data('Anscombe_quartet_data1.csv')\n",
    "data2 = da.Data('Anscombe_quartet_data3.csv')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "data1.plotData()\n",
    "plt.subplot(1,2,2)\n",
    "data2.plotData()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mAJk4uxD0I2"
   },
   "source": [
    "STEP 3: Solving L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7yRhJ1mFTIu"
   },
   "outputs": [],
   "source": [
    "# Define a function that given a matrix \"data\" with two columns and the dregree of data augmentation \"d\" returns: a vector of parameters \"beta\",\n",
    "# the matrix \"X\" of independent variables with dimension (m,d+1), and the matrix of predictions Y. \n",
    "def buildmodel(data,d):\n",
    " \n",
    "  return beta,X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previous function to create your matrices beta Xtr and Ytr for d = 1 for the first dataset and check their shapes\n",
    "beta1,Xtr1,Ytr1 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat it for the second dataset\n",
    "beta2,Xtr2,Ytr2 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4lNKRabi0Rr"
   },
   "outputs": [],
   "source": [
    "# Define a function that given X & Y as inputs returns the analytical solution of the L2 norm\n",
    "def analyticL2(X,Y):\n",
    "  \n",
    "  return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previous function to obtain the optimum beta for Xtr and Ytr for both datasets and print the values\n",
    "beta1 = \n",
    "beta2 = \n",
    "print(f'Optimum beta1 {beta1}\\nOptimum beta2 {beta2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBRF8LZ7ttY7"
   },
   "outputs": [],
   "source": [
    "# This function plots the regression\n",
    "def plotSolution(X,beta,d):\n",
    "  aux = np.zeros((50,X.shape[1]))\n",
    "  aux[:,0] = np.linspace(np.amin(X[:,1]),np.amax(X[:,1]),50)\n",
    "  _,xplot,_ = buildmodel(aux,d)\n",
    "  yplot = xplot@beta\n",
    "  plt.plot(xplot[:,1],yplot,'k')\n",
    "\n",
    "# This function computes the error for the train and test sets\n",
    "def computeErrors(beta,Xtr,Ytr,Xte,Yte):\n",
    "  training_err = np.mean((Ytr - Xtr@beta)**2)\n",
    "  test_err = np.mean((Yte - Xte@beta)**2)\n",
    "  return training_err,test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "128_s5AIXa6o",
    "outputId": "27311474-0c62-41d3-a3bf-f6a4de7f10a6"
   },
   "outputs": [],
   "source": [
    "# We will plot both solutions side to side\n",
    "d = 1\n",
    "plt.figure(figsize = (10,5))\n",
    "# Anscombe1\n",
    "plt.subplot(1,2,1)\n",
    "data1.plotData()\n",
    "plotSolution(Xtr1,beta1,1)\n",
    "\n",
    "# Anscombe3\n",
    "plt.subplot(1,2,2)\n",
    "data2.plotData()\n",
    "plotSolution(Xtr2,beta2,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZbrz3ig7C9p"
   },
   "source": [
    "STEP 4: Solving L1 and Linf norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jn_fUyA2rOAl"
   },
   "outputs": [],
   "source": [
    "# Define two functions that given the parameters from Ax = b, return the coefficients of the variables (c), the left coefficients for the\n",
    "# restrictions (lhs_ineq), and the right coefficients for the restrictions (rhs_ineq)\n",
    "# We are preparing the matrices for the linprog function (Read documentation on linprog)\n",
    "\n",
    "def L1linprog(x,A,b):\n",
    "  \n",
    "  return c,lhs_ineq,rhs_ineq\n",
    "\n",
    "def Linfprog(x,A,b):\n",
    "  \n",
    "  return c,lhs_ineq,rhs_ineq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6KrWpynrwKE",
    "outputId": "37bd9775-ae13-40a2-d6fb-5a74b415ce92"
   },
   "outputs": [],
   "source": [
    "# We build the model again with the second dataset and obtain the solutions for L1 and Linf\n",
    "d = 1\n",
    "beta,Xtr,Ytr = buildmodel(data2.X,d)\n",
    "\n",
    "#L1\n",
    "c,lhs_ineq,rhs_ineq = L1linprog(beta,Xtr,Ytr)\n",
    "print(f'C1 shape {c.shape}, lhs1 shape {lhs_ineq.shape}, rsh1 shape {rhs_ineq.shape}\\n')\n",
    "sol = linprog(c=c, A_ub=lhs_ineq, b_ub=rhs_ineq)\n",
    "betaL1 = sol.x[0:len(beta)]\n",
    "\n",
    "#Linf\n",
    "c,lhs_ineq,rhs_ineq = Linfprog(beta,Xtr,Ytr)\n",
    "print(f'CInf shape {c.shape}, lhsInf shape {lhs_ineq.shape}, rshInf shape {rhs_ineq.shape}\\n')\n",
    "sol = linprog(c=c, A_ub=lhs_ineq, b_ub=rhs_ineq)\n",
    "betaLinf = sol.x[0:len(beta)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "gbSN8jWrhgsx",
    "outputId": "1f922c34-cdb6-4f9b-f8c8-35a76966b629"
   },
   "outputs": [],
   "source": [
    "# Finally we plot the solutions obtained with these two norms\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('L1')\n",
    "data2.plotData()\n",
    "plotSolution(Xtr,betaL1,d)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Linf')\n",
    "data2.plotData()\n",
    "plotSolution(Xtr,betaLinf,d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99YGay9WXj5P"
   },
   "source": [
    "***PART II Regularization***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjutXx_QEXW1"
   },
   "source": [
    "STEP 5: Visualization of underfitting and overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "SGS5f6jmQN7X",
    "outputId": "abd0eca9-0d87-41f4-e62a-40f2d2740f59"
   },
   "outputs": [],
   "source": [
    "# We load the cubic data, and plot the data after scaling it and splitting it\n",
    "cubic = da.Data('P1_cubic.csv')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Raw data')\n",
    "cubic.plotData()\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Scaled & Splitted')\n",
    "cubic.scale()\n",
    "cubic.splitData(0.5)\n",
    "cubic.plotData()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "PD38VU88RVXK",
    "outputId": "f3999410-6d82-40e2-eaec-a92a6c72aba8"
   },
   "outputs": [],
   "source": [
    "# Fit three models with polynomial degree of 1 (underfitting), 3 (fitting), 10 (overfitting).\n",
    "# Hint1: Use build model to create the proper matrices\n",
    "# Hint2: Use analyticL2 to obatin the obtimum parameters\n",
    "\n",
    "# d = 1\n",
    "beta,Xtr,Ytr = \n",
    "beta         = \n",
    "plt.figure(figsize = (12,4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Underfitting')\n",
    "cubic.plotData()\n",
    "plotSolution(Xtr,beta,d)\n",
    "\n",
    "# d = 3\n",
    "beta,Xtr,Ytr = \n",
    "beta         = \n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Fitting')\n",
    "cubic.plotData()\n",
    "plotSolution(Xtr,beta,d)\n",
    "\n",
    "# d = 10\n",
    "beta,Xtr,Ytr = \n",
    "beta         = \n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Overfitting')\n",
    "cubic.plotData()\n",
    "plotSolution(Xtr,beta,d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8To4E4jRh5BU"
   },
   "source": [
    "STEP 6: Applying L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xb9980dIQmiB"
   },
   "outputs": [],
   "source": [
    "# Define a function that given X, Y and l as inputs returns the analytical solution of the regularized L2 norm\n",
    "def analyticL2regularized(X,Y,l):\n",
    "  \n",
    "  return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "txlMAYvt7k9_",
    "outputId": "30d5df23-a700-4353-9694-8a3a1517ebbf"
   },
   "outputs": [],
   "source": [
    "# Use analyticL2 and analyticL2regularized to obtain the optimum parameters and plot the regression for both cases\n",
    "# Hint: Regularize with lambda = 0.0001\n",
    "\n",
    "d = 10\n",
    "beta,Xtr,Ytr = buildmodel(cubic.Xtrain,d)\n",
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "# Non regularized\n",
    "beta = \n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Not regularized')\n",
    "cubic.plotData()\n",
    "plotSolution(Xtr,beta,d)\n",
    "\n",
    "# Regularized l = 0.0001\n",
    "beta = \n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Regularized (L=0.001)')\n",
    "cubic.plotData()\n",
    "plotSolution(Xtr,beta,d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "falE-PcH3RZS"
   },
   "source": [
    "**PART III Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7_auu9A74BN"
   },
   "source": [
    "STEP 7: Analyzing *lambda* with different test ratios (just run to obtain the plots)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "EOGRhpSX3Xs3",
    "outputId": "4cd198f6-2721-4c28-8e4d-709b786f47af"
   },
   "outputs": [],
   "source": [
    "# It is possible to create a loop to iterate for different lambdas and find the optimal one using the test set\n",
    "\n",
    "data = da.Data('P1_cubic.csv')\n",
    "data.scale()\n",
    "d = 10\n",
    "l = np.array([0,10**(-8),10**(-7),10**(-6),10**(-5),10**(-4),10**(-3),10**(-2)])\n",
    "tr = np.array([0.2,0.4,0.6])\n",
    "iter = 200\n",
    "HIST = []\n",
    "Etr = np.zeros((len(l),len(tr)))\n",
    "Ete = np.zeros((len(l),len(tr)))\n",
    "EtrSD = np.zeros((len(l),len(tr)))\n",
    "EteSD = np.zeros((len(l),len(tr)))\n",
    "\n",
    "for j in range(len(tr)):\n",
    "  data.splitData(tr[j])\n",
    "  beta,_,_ = buildmodel(data.Xtrain,d)\n",
    "  betaAUX = np.zeros((len(beta),len(l)))\n",
    "\n",
    "  for i in range(len(l)): \n",
    "    auxtr = np.zeros((iter,))\n",
    "    auxte = np.zeros((iter,))\n",
    "    mask = np.ones((iter,),dtype=int)\n",
    "    for k in range(iter):\n",
    "      data.splitData(tr[j])\n",
    "      _,Xtr,Ytr = buildmodel(data.Xtrain,d)\n",
    "      _,Xte,Yte = buildmodel(data.Xtest,d)\n",
    "      betaAUX[:,i]      = analyticL2regularized(Xtr,Ytr,l[i])\n",
    "      auxtr[k],auxte[k] = computeErrors(betaAUX[:,i],Xtr,Ytr,Xte,Yte)\n",
    "    Etr[i,j] = np.nanmean(auxtr)\n",
    "    Ete[i,j] = np.nanmean(auxte)\n",
    "    EtrSD[i,j] = np.std(auxtr)\n",
    "    EteSD[i,j] = np.std(auxte)\n",
    "  HIST.append(betaAUX)\n",
    "\n",
    "plt.figure(figsize = (4*len(l),4))\n",
    "for i in range(len(l)):\n",
    "  plt.subplot(1,len(l),i+1)\n",
    "  data.plotData()\n",
    "  plotSolution(Xtr,HIST[-1][:,i],d)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "2g4PjyGBEVzI",
    "outputId": "55d83b47-cb5c-4044-feca-bc206f7e109b"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "for j in range(len(tr)):\n",
    "#  plt.errorbar(l,Etr[:,j],yerr=EtrSD[:,j],fmt='-o')\n",
    "  plt.plot(l,Etr[:,j],'-o')\n",
    "plt.title('Training error vs lambda')\n",
    "plt.legend(['20% test','40% test','60% test'])\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "for j in range(len(tr)):\n",
    "#  plt.errorbar(l,Ete[:,j],yerr=EteSD[:,j],fmt='-o')\n",
    "  plt.plot(l,Ete[:,j],'-o')\n",
    "plt.title('Test error vs lambda')\n",
    "plt.legend(['20% test','40% test','60% test'])\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "plt.ylim((10**-4,10**1))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLYpobCbF5jp"
   },
   "source": [
    "STEP 8: Analyzing *lambda* with different polynomial degrees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "_ZvzJhDDrhLu",
    "outputId": "723262cf-a21f-4fe5-bc47-57f0d9b2087f"
   },
   "outputs": [],
   "source": [
    "data = da.Data('P1_cubic.csv')\n",
    "data.scale()\n",
    "tr = 0.5\n",
    "d = np.array([3,6,10])\n",
    "l = np.array([0,10**(-8),10**(-7),10**(-6),10**(-5),10**(-4),10**(-3),10**(-2)])\n",
    "iter = 100\n",
    "Etr = np.zeros((len(l),len(d)))\n",
    "Ete = np.zeros((len(l),len(d)))\n",
    "EtrSD = np.zeros((len(l),len(d)))\n",
    "EteSD = np.zeros((len(l),len(d)))\n",
    "\n",
    "for j in range(len(d)):\n",
    "  betaAUX = np.zeros((d[j]+1,len(l)))\n",
    "  for i in range(len(l)): \n",
    "    auxtr = np.zeros((iter,))\n",
    "    auxte = np.zeros((iter,))\n",
    "    for k in range(iter):\n",
    "      data.splitData(tr)\n",
    "      _,Xtr,Ytr = buildmodel(data.Xtrain,d[j])\n",
    "      _,Xte,Yte = buildmodel(data.Xtest,d[j])\n",
    "      betaAUX[:,i]      = analyticL2regularized(Xtr,Ytr,l[i])\n",
    "      auxtr[k],auxte[k] = computeErrors(betaAUX[:,i],Xtr,Ytr,Xte,Yte)\n",
    "    Etr[i,j] = np.mean(auxtr)\n",
    "    Ete[i,j] = np.mean(auxte)\n",
    "    EtrSD[i,j] = np.std(auxtr)\n",
    "    EteSD[i,j] = np.std(auxte)\n",
    "plt.figure(figsize = (4*len(l),4))\n",
    "for i in range(len(l)):\n",
    "  plt.subplot(1,len(l),i+1)\n",
    "  data.plotData()\n",
    "  plotSolution(Xtr,betaAUX[:,i],d[-1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "8zcB9btrsHw3",
    "outputId": "be1c4bc0-da2c-4410-f384-8cf8b9a2ed95"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "for j in range(len(d)):\n",
    "#  plt.errorbar(l,Etr[:,j],yerr=EtrSD[:,j],fmt='-o')\n",
    "  plt.plot(l,Etr[:,j],'-o')\n",
    "plt.title('Training error vs lambda')\n",
    "plt.legend(['d = 3','d = 6','d = 10'])\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "for j in range(len(d)):\n",
    "#  plt.errorbar(l,Ete[:,j],yerr=EteSD[:,j],fmt='-o')\n",
    "  plt.plot(l,Ete[:,j],'-o')\n",
    "plt.title('Test error vs lambda')\n",
    "plt.legend(['d = 3','d = 6','d = 10'])\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "plt.ylim((10**-4,10**-1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "ZY68Gg8xIpFB",
    "outputId": "27a6d27a-4647-439f-c340-f09105f45d08"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1-LinearRegression(Solution).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

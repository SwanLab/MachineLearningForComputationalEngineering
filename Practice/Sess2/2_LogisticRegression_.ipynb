{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osMjs8BUxiAV"
   },
   "source": [
    "#PRACTICAL SESSION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1rgWra2xxzH"
   },
   "source": [
    "**PART I LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLLXsKNZyEJr"
   },
   "source": [
    "STEP 1: Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nU6EOoUjJVhg"
   },
   "outputs": [],
   "source": [
    "# Important libraries\n",
    "import csv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "from data_analysis import Data,buildmodel,PlotBoundary,testAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RmytBvSyFkS"
   },
   "source": [
    "STEP 2: Defining a class to create usable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Df-o-Nn_27SS"
   },
   "outputs": [],
   "source": [
    "# Lets create the object data and plot the correlation matrix\n",
    "data = Data('P2_microchip_logreg.csv')\n",
    "data.splitData(0.2)\n",
    "data.plotCorrelationMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obj-jr15yfrH"
   },
   "source": [
    "STEP 3: Solving the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "128_s5AIXa6o"
   },
   "outputs": [],
   "source": [
    "# Define the hypothesis function between X and theta\n",
    "def hypothesisFunction(X,theta):\n",
    "  \n",
    "  return h\n",
    "\n",
    "# Define a function that applies sigmoid element-wise\n",
    "def sigmoid(z):\n",
    "\n",
    "  return g\n",
    "\n",
    "# Write a function that returns a scalar with inputs: predictions \"P\" and targets \"T\", using negative loglikelihood \n",
    "def crossentroy(P,T):\n",
    "  \n",
    "  return L\n",
    "\n",
    "# Define the function that given the parameter lambda and theta returns the regularization term\n",
    "def regularization(l,theta):\n",
    "  \n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmKrdvi7zRZs"
   },
   "outputs": [],
   "source": [
    "# Gather all the functions above to create a function that given a set of parameters (theta, the data X, the labels Y and the value of \n",
    "# lambda), it returns the cost of the negative loglikelihood and another function which returns its gradient.\n",
    "def costFunction(theta,X,Y,l):\n",
    "  \n",
    "  return J\n",
    "  \n",
    "def gradient(theta,X,Y,l):\n",
    "  \n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OJQ6EIHQ1zi"
   },
   "outputs": [],
   "source": [
    "# We use build model to create the desired matrices both for train and test. Then minimize function from scipy is used to \n",
    "# to obtain the optimum parameters\n",
    "# Try different values of data augmentation to observe the boundaries\n",
    "d = 1\n",
    "l = 0\n",
    "theta0,Xtr,Ytr = buildmodel(data.Xtrain,data.Ytrain,d,[0,1])\n",
    "_,Xte,Yte = buildmodel(data.Xtest,data.Ytest,d,[0,1])\n",
    "sol = minimize(costFunction,theta0,args=(Xtr,Ytr,l))\n",
    "theta = np.reshape(sol.x,(Xtr.shape[1],Ytr.shape[1]))\n",
    "trA = testAccuracy(Xtr,Ytr,theta)\n",
    "teA = testAccuracy(Xte,Yte,theta)\n",
    "print('\\nTrain accuracy: ',trA,'\\nTest accuracy: ',teA)\n",
    "PlotBoundary(data,theta,d,[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjfXuRy0285N"
   },
   "source": [
    "STEP 4: Applying regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxKT26at28VT"
   },
   "outputs": [],
   "source": [
    "# Play around with the lambda parameter with a high value of d\n",
    "d = 1\n",
    "l = 0\n",
    "theta0,Xtr,Ytr = buildmodel(data.Xtrain,data.Ytrain,d,[0,1])\n",
    "_,Xte,Yte = buildmodel(data.Xtest,data.Ytest,d,[0,1])\n",
    "sol = minimize(costFunction,theta0,args=(Xtr,Ytr,l))\n",
    "theta = np.reshape(sol.x,(Xtr.shape[1],Ytr.shape[1]))\n",
    "trA = testAccuracy(Xtr,Ytr,theta)\n",
    "teA = testAccuracy(Xte,Yte,theta)\n",
    "print('\\nTrain accuracy: ',trA,'\\nTest accuracy: ',teA)\n",
    "PlotBoundary(data,theta,d,[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-Hf-a17dbWt"
   },
   "source": [
    "**GRADIENT DESCENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy65DfuM5WTo"
   },
   "source": [
    "STEP 5: Implementing gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTELjavQbjKu"
   },
   "outputs": [],
   "source": [
    "# We create a class gradient descent to use our own optimizer\n",
    "# We have to define two main things, 1)the step 2)create the loop in the train method\n",
    "class gradientDescent:\n",
    "  \n",
    "  def __init__(self,lr,maxiters,plot):\n",
    "    self.lr = lr\n",
    "    self.maxIters = maxiters\n",
    "    self.plot = plot\n",
    "    self.optHIST = []\n",
    "\n",
    "  def train(self,F,f,X,Y,theta):\n",
    "    # Loop\n",
    "    \n",
    "    self.optHIST = np.array(self.optHIST)\n",
    "    self.plotEnd()\n",
    "    return theta\n",
    "\n",
    "  def step(self,theta,grad):\n",
    "    #Step\n",
    "    \n",
    "    return theta\n",
    "      \n",
    "  def plotEnd(self):\n",
    "    # A function that plots at the end the evolution of the function value and the step size\n",
    "    if (self.plot == True):\n",
    "      plt.show()\n",
    "      plt.figure(1,figsize=(10,5))\n",
    "      s = np.arange(0,self.maxIters,20)\n",
    "      plt.subplot(1,2,1)\n",
    "      plt.title('Function value vs iter')\n",
    "      plt.scatter(s,self.optHIST[s,0])\n",
    "      plt.subplot(1,2,2)\n",
    "      plt.title('Step size vs iter')\n",
    "      plt.scatter(s,self.optHIST[s,1])\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKq3xhi1fPI1"
   },
   "outputs": [],
   "source": [
    "# We try our minimizer\n",
    "# Create the matrices and the gradient descent object\n",
    "d = 2\n",
    "l = 0.0\n",
    "theta0,Xtr,Ytr = buildmodel(data.Xtrain,data.Ytrain,d,[0,1])\n",
    "gd = gradientDescent(lr=0.01,maxiters=500,plot=True)\n",
    "\n",
    "# Create the cost and gradient as handle functions to give them to our minimizer\n",
    "F = lambda theta,X,Y: costFunction(theta,X,Y,l)\n",
    "f = lambda theta,X,Y: gradient(theta,X,Y,l)\n",
    "\n",
    "sol = gd.train(F,f,Xtr,Ytr,theta0)\n",
    "theta = np.reshape(sol,[Xtr.shape[1],Ytr.shape[1]])\n",
    "PlotBoundary(data,theta,d,[0,1])\n",
    "\n",
    "_,Xte,Yte = buildmodel(data.Xtest,data.Ytest,d,[0,1])\n",
    "tr        = testAccuracy(Xtr,Ytr,theta)\n",
    "te        = testAccuracy(Xte,Yte,theta)\n",
    "print('Train accuracy: ',tr,'\\nTest accuracy: ',te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1nzRhI35f1T"
   },
   "source": [
    "STEP 7: Implementing stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQ80YtPuG0y5"
   },
   "outputs": [],
   "source": [
    "# We can create a class stochastic gradient descent which inherits from gradient descent\n",
    "# We overwrite the loop (notice the only change is another loop inside iterating through the bath)\n",
    "# We create a method create minibatch wich will create the batches at each iteration\n",
    "class SGD(gradientDescent):\n",
    "  def __init__(self,lr,maxepochs,batch,plot):\n",
    "    gradientDescent.__init__(self,lr,maxepochs,plot)\n",
    "    self.batchSz = batch\n",
    "\n",
    "  def train(self,F,f,X,Y,theta,outFCN):\n",
    "    # Here occurs the main loop of the minimization\n",
    "    epoch = 0\n",
    "    iter  = 0\n",
    "    nB    = X.shape[0]//self.batchSz\n",
    "    while epoch < self.maxIters:\n",
    "      if (nB == 1 or nB == 0):\n",
    "        order = np.arange(X.shape[0])\n",
    "        nB = 1;\n",
    "      else:\n",
    "        order = np.random.permutation(X.shape[0])\n",
    "      for i in range(nB):\n",
    "        Xb,Yb = self.createMinibatch(X,Y,order,i)\n",
    "        J     = F(theta,Xb,Yb)\n",
    "        grad  = f(theta,Xb,Yb)\n",
    "        gnorm = np.linalg.norm(grad)\n",
    "        theta  = self.step(theta,grad)\n",
    "        iter = iter + 1\n",
    "      self.optHIST.append([J,gnorm*self.lr])\n",
    "      epoch  += 1\n",
    "    self.optHIST = np.array(self.optHIST)\n",
    "    self.plotEnd()\n",
    "    return theta\n",
    "\n",
    "  def createMinibatch(self,X,Y,order,i):\n",
    "    # Function that creates the minibatch\n",
    "    cont = 0\n",
    "    bs = self.batchSz\n",
    "    if (i == X.shape[0]//bs-1):\n",
    "      plus = X.shape[0]%bs\n",
    "    else:\n",
    "      plus = 0\n",
    "    Xb = X[order[i*bs:(i+1)*bs+plus],:]\n",
    "    Yb = Y[order[i*bs:(i+1)*bs+plus],:]\n",
    "    return Xb,Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7hEYWrGN5lQ"
   },
   "outputs": [],
   "source": [
    "# We test our new minimizer\n",
    "d = 2\n",
    "l = 0.0\n",
    "idx = [0,1]\n",
    "m = data.Xtrain.shape[0]\n",
    "theta0,Xtr,Ytr = buildmodel(data.Xtrain,data.Ytrain,d,idx)\n",
    "F = lambda theta,X,Y: costFunction(theta,X,Y,l)\n",
    "f = lambda theta,X,Y: gradient(theta,X,Y,l)\n",
    "sgd = SGD(lr=0.1,maxepochs=200,batch=m,plot=True)\n",
    "sol = sgd.train(F,f,Xtr,Ytr,theta0,outFCN)\n",
    "theta = np.reshape(sol,[Xtr.shape[1],Ytr.shape[1]])\n",
    "\n",
    "PlotBoundary(data,theta,d,idx)\n",
    "_,Xte,Yte = buildmodel(data.Xtest,data.Ytest,d,idx)\n",
    "tr        = testAccuracy(Xtr,Ytr,theta)\n",
    "te        = testAccuracy(Xte,Yte,theta)\n",
    "print('Train accuracy: ',tr,'\\nTest accuracy: ',te)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2-LogisticRegression_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

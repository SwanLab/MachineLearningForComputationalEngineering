<html>
<head>
    <meta charset="UTF-8">
    <meta name ="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning for Computational Engineering</title>
    <meta name="description" content="Swan is an open-source topology optimization toolbox capable of performing structural and material design.">
    <link rel="stylesheet" href="styles.css">
    <link href='https://fonts.googleapis.com/css?family=Poppins:400,600,700' rel='stylesheet'>
    <link rel="shortcut icon" type="image/png" href="website/images/Favicon.png">
</head>

<body>
    <div class="swanhead">
        <div class="top-navbar">
            <a class="logo">
                <img src="website/images/swanlogo_black.png" height="45px">
            </a>
            <a href="https://github.com/SwanLab/Swan/" class="ghlogo">
                <img src="website/images/GitHub-Mark-32px.png"" height="28px">
            </a>
            <a href="#contributors" class="top-navbar-link">Contributors</a>
            <a href="#examples" class="top-navbar-link">Examples</a>
            <a href="#features" class="top-navbar-link">Features</a>
        </div>

        
        <div class="swanhead-content">

        <div class="swanhead-features">
            <div class="swanhead-features-LHS">
                <div class="swanhead-text">
                    <div>
                        <div class="headline">
                            Machine Learning for Computational Engineering
                        </div>
                        <div class="headline-sub">
                            Join us in this free    course where the travellers will have a first glance at some basic aspects on machine learning. A basic introduction on optimization and statistics will fill the traveller backpack to better enjoy the machine learning journey. For a prompt integration of the concepts described, these will be presented in a Computational Engineering language. The practical sessions of the course will allow the travellers to become familiar with the basic machine learning culture.  
                        </div>
                    </div>
        
                </div>
            </div>
            <div class="swanhead-features-RHS">
                <!-- Its underwhelming to say the least-->
                <img src="Practice/optgif.gif" height="350px"/>
            </div>
            <div class="swanhead-features-RHS">
            </div>
        </div>
        </div>
    </div>

    <div class="swancontent">

    <!-- SWAN: CONTRIBUTORS -->
    <div class="section-start">
        <a id="contributors"></a>
        <div class="section-pre">Who</div>
        <div class="section-header">Meet the teachers</div>
    </div>

    <div class="meet-the-teachers-carousel">
        <div class="meet-the-teachers-carousel-image">
            <img src="images/fullhouse.jpeg" height="100%"/>
            <img src="images/session1.jpeg" height="100%"/>
            <img src="images/sessionPractice.jpeg" height="100%"/>
            <img src="images/bonet.jpeg" height="100%"/>
            <img src="images/sessionPractice2.jpeg" height="100%"/>
        </div>
    </div>

    <div class="section-contributors">
        <div class="section-contributors-contributor" >
            <div class="contributor-image"> <img src="teachers/alexferrer.png"/></div>
            <div class="contributor-name">Àlex Ferrer</div>
            <div class="contributor-text">Researcher at CIMNE and lecturer at UPC, Àlex defended <a href="https://upcommons.upc.edu/handle/2117/108498">his thesis</a> on multi-scale topological design of materials</div>
        </div>

        <div class="section-contributors-contributor" >
            <div class="contributor-image"> <img src="teachers/toni.png"/></div>
            <div class="contributor-name">Toni Darder</div>
            <div class="contributor-text">As part of <a href="https://upcommons.upc.edu/handle/2117/188789">his dissertation</a>, Ferran worked on topology optimization with a focus on manufacturability </div>
        </div>
    </div> <!-- end of contributors -->

    <!-- SWAN: FEATURES -->
    
    <div class="section-start">
        <a id="features"></a>
        <div class="section-pre">What</div>
        <div class="section-header">What you'll learn</div>
    </div>

    <div class="swancontent-features">
        <div class="swancontent-features-LHS">

            <!--- Material design -->
            <div class="swancontent-feature"> 
                <div class="swancontent-feature-title"> Material design </div>
                <div class="swancontent-feature-contents">
                    Swan can perform analyses of microstructures: when combining it with its topology optimization capabilities, 
                    <span class="feature-highlight">novel metamaterials</span> can be designed in order to tackle complex
                    challenges and further push the boundaries of engineering.
                    
                </div>
            </div>

            <!--- Structural design -->
            <div class="swancontent-feature"> 
                <div class="swancontent-feature-title"> Structural design </div>
                <div class="swancontent-feature-contents">
                    The modular design of Swan allows the combination of several functionals in order
                    to <span class="feature-highlight">define complex optimization problems</span>. Among the functionals that can be used as constraints are compliance, volume,
                    and perimeter. Swan also features <span class="feature-highlight">density-based optimizers</span> like Projected Gradient, MMA and IPOPT,
                    as well as <span class="feature-highlight">level-set methods</span> such as SLERP, Projected SLERP and Hamilton-Jacobi.
                </div>
            </div>
 
            <!--- Multi-scale -->
            <div class="swancontent-feature"> 
                <div class="swancontent-feature-title"> Multi-scale </div>
                <div class="swancontent-feature-contents">
                    One of the key features that sets Swan apart from other topology optimization
                    toolboxes is the ability to design <span class="feature-highlight">optimal materials at the micro scale</span>, and reuse
                    the obtained results to perform analyses at the <span class="feature-highlight">macro level</span>.
                </div>
            </div>
            
            <!--- Its coming up -->
            <div class="swancontent-feature"> 
                <div class="swancontent-feature-title"> Multiphysics, and much more </div>
                <div class="swancontent-feature-contents">
                    We are constantly looking ahead and recruiting new contributors in order to keep
                    expanding Swan's capabilities. Among the planned upcoming features are <span class="feature-highlight">multiphysics</span>,
                    <span class="feature-highlight"></span>3D microstructural optimization</span>, and many more.
                </div>
            </div>

        </div>

        <div class="swancontent-features-RHS">
            <img src="website/images/slideshow/mix.png" height="500px"/>
            <!--
            <img src="website/images/slideshow/image1.jpg" height="500px"/>
            <img src="website/images/slideshow/image2.jpg" height="500px"/>
            <img src="website/images/slideshow/image3.jpg" height="500px"/>
            -->
            <!--
            <div class="image-slider">
                <ul class="image-slider__img">
                  <li class="image-slider__img"><img src="website/images/slideshow/image1.jpg" height="500px"/></li>
                  <li class="image-slider__img"><img src="website/images/slideshow/image2.jpg" height="500px"/></li>
                  <li class="image-slider__img"><img src="website/images/slideshow/image3.jpg" height="500px"/></li>
                  <li class="image-slider__img"><img src="website/images/slideshow/image1.jpg" height="500px"/></li>
                </ul>
            </div>
            -->
            <div class="swancontent-image-description">
                Optimization of the 3D cantilever benchmark
            </div>
        </div>
    </div>

    <!-- SWAN: EXAMPLES -->
    <div class="section-start">
        <a id="examples"></a>
        <div class="section-pre">What</div>
        <div class="section-header">Lessons</div>
    </div>

    <div class="section-examples-previa">Here are some examples of what we have achieved at Swan.</div>
    
    <div class="section-examples">

        <div class ="section-newExample">
            <div class="section-newExample-imgHorizontal">
                <iframe width="100%" height="500vh" src="https://www.youtube.com/embed/ssrCbiaKuJU" title="Basics in continuous optimization Part I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div class="section-newExample-content">
                <div class="section-newExample-tags">
                    <span class="tag-example"><a href="https://drive.google.com/file/d/1L0KJDkWPi2UMBKwauWvRo1yIIuRDYYNK/view">Lecture notes</a></span>
                </div>
                <div class="examples-name">Basics in continuous optimization (Part I)</div>
                <div class="examples-text">
                    An introduction to continuous optimization will be presented followed by some basic examples on least squares and linear (and sequential quadratic) programming. Then, unconstrained optimization will be discussed jointly with gradient-based algorithms. This module will end presenting some line-search methods (or learning rates) jointly with some illustrative examples.

                </div>
            </div>
        </div>

        <div class ="section-newExample">
            <div class="section-newExample-imgHorizontal">
                <iframe width="100%" height="500vh" src="https://www.youtube.com/embed/ssrCbiaKuJU" title="Basics in continuous optimization Part I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div class="section-newExample-content">
                <div class="section-newExample-tags">
                    <span class="tag-example"><a href="https://drive.google.com/file/d/1oBWZCdeCrBcYBB9Zg0q8gHewpahJSgeG/view">Lecture notes</a></span>
                </div>
                <div class="examples-name">Applications in Machine Learning</div>
                <div class="examples-text">
                    This module will serve as a first taste on machine learning. We will present the similarities and differences between machine learning and classical optimization. First examples on regression will allow us to internalise the optimization concepts explained in the first sessions. Mean square errors, sparse approximation, lasso problems, ridge regression, l1 norm optimization will be some examples.
                </div>
            </div>
        </div>

        <div class ="section-newExample">
            <div class="section-newExample-imgHorizontal">
                <iframe width="100%" height="500vh" src="https://www.youtube.com/embed/GNkfw74mxs0" title="Basics in continuous optimization Part II" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div class="section-newExample-content">
                <div class="section-newExample-tags">
                    <span class="tag-example"><a href="https://drive.google.com/file/d/1KlP0PSK3v0_SsNN69vqiuifxOAhzgoNV/view">Lecture notes</a></span>
                </div>
                <div class="examples-name">Basics in continuous optimization (Part II)</div>
                <div class="examples-text">
                    This second excursion to continuous optimization will be more focused on constrained optimization problems where duality will play a central role. Legendre-fenchel transform, lagrange multipliers, penalty methods, KKT conditions and other concepts in constrained optimization will be described. Useful algorithms like SQP, augmented lagrangian, trust-region and interior point methods will be presented. The theory will be assisted by several illustrative examples.
                </div>
            </div>
        </div>

        <div class ="section-newExample">
            <div class="section-newExample-imgHorizontal">
                <iframe width="100%" height="500vh" src="https://www.youtube.com/embed/ScazFBZ9v48" title=" Supervised Learning Part I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div class="section-newExample-content">
                <div class="section-newExample-tags">
                    <span class="tag-example"><a href="https://drive.google.com/file/d/1zJ1IxcEdhgceatM9tDYsFWrpFDqXe3q-/view">Lecture notes</a></span>
                </div>
                <div class="examples-name">Supervised Learning (Part I)</div>
                <div class="examples-text">
                    We will start with polynomial regression to understand the capacity of the machine learning models. We will analyze the underfitting-overfitting trade-off and the importance of regularisation and the hyperparameters. In this module, we will also present logistic regression and the main classification models including multi-classification. We will end by introducing the support vector machine problem.
                </div>
            </div>
        </div>

        <div class ="section-newExample">
            <div class="section-newExample-imgHorizontal">
                <iframe width="100%" height="500vh" src="https://www.youtube.com/embed/WLxJLF0PLCk" title=" Optimization in Computational Engineering " frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div class="section-newExample-content">
                <div class="section-newExample-tags">
                    <span class="tag-example"><a href="https://drive.google.com/file/d/11Mos0r-fjKfgP5c1fdWulLokaApPvNGp/view">Lecture notes</a></span>
                </div>
                <div class="examples-name">Optimization in Computational Engineering</div>
                <div class="examples-text">
                    We will start with polynomial regression to understand the capacity of the machine learning models. We will analyze the underfitting-overfitting trade-off and the importance of regularisation and the hyperparameters. In this module, we will also present logistic regression and the main classification models including multi-classification. We will end by introducing the support vector machine problem.
                </div>
            </div>
        </div>

        <div class ="section-newExample">
            <div class="section-newExample-imgHorizontal">
                <iframe width="100%" height="500vh" src="https://www.youtube.com/embed/DsSGEmHjrD0" title="Supervised Learning II" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div class="section-newExample-content">
                <div class="section-newExample-tags">
                    <span class="tag-example"><a href="https://drive.google.com/file/d/1UlmTaZziwN4Sh5P5gBxklwNljSljXHw0/view">Lecture notes</a></span>
                </div>
                <div class="examples-name">Supervised Learning (Part II)</div>
                <div class="examples-text">
                    We will start with polynomial regression to understand the capacity of the machine learning models. We will analyze the underfitting-overfitting trade-off and the importance of regularisation and the hyperparameters. In this module, we will also present logistic regression and the main classification models including multi-classification. We will end by introducing the support vector machine problem.
                </div>
            </div>
        </div>

        <div class ="section-newExample">
            <div class="section-newExample-imgHorizontal">
                <iframe width="100%" height="500vh" src="https://www.youtube.com/embed/Kt-72Tdfsiw" title="Statistical Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div class="section-newExample-content">
                <div class="section-newExample-tags">
                    <span class="tag-example"><a href="https://drive.google.com/file/d/1RG7jFC8OgCVvtyuE2ZmGxZglv9kTlJik/view">Lecture notes</a></span>
                </div>
                <div class="examples-name">Statistical Learning</div>
                <div class="examples-text">
                    Machine learning requires some important concepts from statistics. In this module, the necessary ingredients of statistics will be gradually introduced. We will move from reviewing basic concepts from probability and statistics to the study of bayesian inference and some useful parameter estimators. The maximum likelihood estimator and maximum a posteriori estimation will allow us to provide a statistical insight to regression and classification models.
                </div>
            </div>
        </div>

        <div class ="section-newExample">
            <div class="section-newExample-imgHorizontal">
                <iframe width="100%" height="500vh" src="https://www.youtube.com/embed/PDAPeRW1rDg" title="Unsupervised Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div class="section-newExample-content">
                <div class="section-newExample-tags">
                    <span class="tag-example"><a href="https://drive.google.com/file/d/1K0--58kMdVm26-qNdc6RQsccEqckZNRo/view">Lecture notes</a></span>
                </div>
                <div class="examples-name">Unsupervised Learning</div>
                <div class="examples-text">
                    We will present in this module the clustering problem to motivate the importance of unsupervised learning. We will start with the K-means algorithm and the expectation maximization algorithm. Then, we will present the PCA algorithm for dimensional reduction. We will end presenting some more advanced unsupervised learning like independent component analysis (ICA) and non-negative matrix factorization problems.
                </div>
            </div>
        </div>
    </div>


    <!--tfgs/tfms guais + posar algo de projectes europeus?-->
    
    
    <!-- SWAN: REVIEWS -->
    
    <div class="section-start">
        <a id="reviews"></a>
        <div class="section-pre">Why</div>
        <div class="section-header">Reviews</div>
    </div>


    <div class="meet-the-teachers-carousel">
        <div class="meet-the-teachers-carousel-image">
            <img src="survey/veryinteresting.png" height="100%"/>
            <img src="survey/useful.png" height="100%"/>
            <img src="survey/ratelevel.png" height="100%"/>
            <img src="survey/teachers.png" height="100%"/>
            <img src="survey/likedtheoretical.png" height="100%"/>
            <img src="survey/likedpractical.png" height="100%"/>
        </div>
    </div>
    
    "I am very satisfied with the Summer School, the theorical part was very clear and the exercises were reasonable to do."
    "I think the format is great!"

    </div>
</body>